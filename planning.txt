------------------------ Task 1: Config Parser Design ------------------------ 
Write pseudocode for a function that:

def load_config(config_path):
    """
    Load and validate the YAML configuration.
    
    What validation checks should happen here?
    What errors might occur?
    What should the return type be?
    """
    pass

This function is responsible for laoding the datasets.yaml configuration for each dataset that is specified and required to be downloaded. The logic of the function should be as follows:
1. Perform validation checks on the path, these should include - checking the name and URL is available for the dataset, chekc if the file is required to be extracted after download, checking the destination folder exists, make sure that the checksum is valid. ‚úÖ
2. Errors that might occur include - dataset not found due to naming errors, fields missing and i am not sure what else could cause errors.
3. I am assuming the return type should be a dictionary where the keys are the dataset names and the values are a dictionary of the other values in the config file such as URL, destination folder, extract after download, etc. ‚úÖ


------------------------ Task 1: Config Parser Design ‚úÖ (Mostly Correct!) ------------------------

Your logic is good, but let's refine:
What you got right:

Validation checks are spot-on ‚úÖ
Dictionary return type is correct ‚úÖ

What to improve:

def load_config(config_path):
    """
    Questions to think about:
    
    1. What if config_path doesn't exist?
       ‚Üí Should you raise FileNotFoundError immediately?
       --> Yes i think we should raise the error immediately to avoid going through any more of the project with the error becasue if the config_path doesn't exist then the rest of teh project will likely casue more errors such as, trying to find the URL of the path that doesn't exist. 
       
    2. What if YAML is malformed?
       ‚Üí yaml.safe_load() can raise YAMLError - catch it?
       --> I think we should catch this error as well. Like i said in the previous question, if there is a problem with the .yaml file then we will run into more problems later on in the project.
       
    3. What if 'datasets' key is missing?
       ‚Üí Is this a fatal error or return empty list?
       --> I think this is fatal error as it is the main "heading" for the strucutre of the datasets that we are required to downlaod. So if this is missing then the project will not be able to download any files. However we could return an empty list and then the project does not proceed to download any files. I am not sure how we can hanlde this error. 

       üí°Teacher guidance:
       # Option 1: Fatal error (my recommendation)
       if 'datasets' not in config:
            raise ValueError("Config must have 'datasets' key")

       # Option 2: Empty list (too permissive)
       datasets = config.get('datasets', [])  # Fails silently - bad!
       # WHY fatal? If someone loads empty config, they expect an error, not silent success.
       
    4. Validation depth: Do you validate checksums are valid hex?
       ‚Üí Or just check the field exists?
       --> We should make sure that the checksums are valid hex or else the input for the checksum field could be a bunch on nonsense. 

       üí°Teacher guidance:
       import re

        def is_valid_md5(checksum: str) -> bool:
            return bool(re.match(r'^[a-fA-F0-9]{32}$', checksum))

        def is_valid_sha256(checksum: str) -> bool:
            return bool(re.match(r'^[a-fA-F0-9]{64}$', checksum))
       
    5. Return structure: Should it be a dict or a list of Dataset objects?
       ‚Üí Dict[str, dict] or List[DatasetConfig]?
       --> I think for clarity a dictionary would make it easier to follow which objects we require and use throughout the project.

       üí°Teacher guidance:
       # Option A: Dict[str, DatasetConfig]
        configs = load_config('datasets.yaml')
        cifar = configs['cifar10']  # Easy lookup by name

        # Option B: List[DatasetConfig]  
        configs = load_config('datasets.yaml')
        for dataset in configs:  # Easy iteration for batch downloads
            download(dataset)

        # Which matches your use case better?
        # ‚Üí Downloading multiple datasets = iterating = List! 
    """
Challenge for you: Should validation happen IN load_config() or in a separate validate_config() function? What's more testable?
--> I think we should create a separate validate_config() function, this makes the code more modular and testable since we can tweak the validate_config() function to check various validation steps without adding to many steps and lines of code to the load_config() function.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


------------------------ Task 2: Logging Strategy ------------------------

Write out what you'd log at each stage:

Starting a download?
- Log which file is being downloaded (name or URL)
- Time download started

Thread spawned?
- Notify when a new thread has been spawned and what task it is allocated

Chunk downloaded?
- Notify when chunk downloader is being used 
- Identify the chunk size of the downloads

Download complete?
- Notify the download has completed 
- Identify where the file has been saved
- Calculate how long the download took

Validation failed?
- What was the reason for validation failing 
- Log request errors
- Log which file caused the error(s)

Think: In production, you might download 100 files. How do you make logs searchable and useful?
- Logs should be organised by the file they represent

------------------------ Task 2: Logging Strategy ‚úÖ (Good, but missing details!) ------------------------

You're thinking right, but let's be more specific:

# What EXACTLY should the log messages look like?

# ‚ùå Vague:
logger.info("Download started")

# ‚úÖ Production-ready:
logger.info(f"[{dataset_name}] Starting download: {url} ({file_size_mb:.2f} MB)")

# Thread safety consideration:
logger.info(f"[Thread-{thread_id}] [{dataset_name}] Downloaded chunk {chunk_num}/{total_chunks}")

Your statement: "Logs should be organised by the file they represent"

Question: How do you organize logs by file when they're all in one downloader.log?
--> I am not sure how we would organise logs by file. I have never worked with logging files before so it this project is a great learnig step for me. In saying this i am not sure what the best method for saving logging the files would be especially if we want to make them searchable.

üí° Options:

Prefix every log with [dataset_name] (easy to grep)
Separate log file per dataset (harder to debug concurrent issues)
Structured logging (JSON format - industry standard!)

Challenge: Research "structured logging" and consider if it's worth implementing.
--> Structure logging makes use of the "structlog" module and allows you to split up the log entries into key/value pairs with incremental building. It also removes the hard work of writing hard-to-parse and hard-to-keep-consistent prose in your logs. I think this could be a good idea for this project. 

üí° Teacher guidance:

Three Approaches:

1. Prefix logging (Simplest - Let's use this):

logger.info(f"[cifar10] Starting download from {url}")
logger.info(f"[cifar10] [Thread-3] Downloaded chunk 5/10")

# Grep logs later:
# grep "\[cifar10\]" downloader.log

2. Structured logging (Industry standard - Future enhancement):

import structlog

log = structlog.get_logger()
log.info("download_started", dataset="cifar10", url="...", size_mb=170)

# Output as JSON:
{"event": "download_started", "dataset": "cifar10", "url": "...", "timestamp": "..."}

3. Separate log files (Overkill for now):

logs/cifar10.log
logs/mnist.log

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


------------------------ Task 3: Error Scenarios ------------------------

List 5 things that could go wrong during downloads and how you'd handle each:

Network timeout ‚Üí Log that the network has timed out, know the progress of the download when the timeout occurred. Wait a certain amount of time and then resume the download from where the timeout occurred.

Disk full ‚Üí If disk is full, notify the user. Check for old or outdated logs and ask the use if it would be okay to remove them. Do not continue download if they file size will cause the disk to become full.

Invalid URL ‚Üí Log the request status (404, 4.., 200, etc), notify the user the URL is invalid with the current URL. Ask user to make sure the URL is correct before the program continues.

Checksum mismatch ‚Üí Notify the user that the checksum is invalid and ask if they would like to re-download the file, if yes - remove the downlaod and try again, if no - ask if the user would like to remove the corrupted file or not. Also ask user to check their internet connection or use a different download source if the problem persists.

Partial download (resume scenario) ‚Üí Continously log the progress of the download, if the download fails log the progess up until the failure. then notify the user and ask if they would like to try download the file again. Try and download from where the progess failed. 

------------------------ Task 3: Error Scenarios ‚úÖ (Excellent thinking!) ------------------------

Your error handling is thoughtful, but some refinements:

Network timeout:

"Wait a certain amount of time and then resume"

Teacher question:

How long do you wait? Exponential backoff? (Look this up!)
How many retries before giving up?
What if the server is down for hours?
--> I do not think we should use Exponential backoff when we have a timeout or network failure. As you said the server could be down for hours so there would be no point in using the Exponential backoff for this project. We should set a standard waiting time and a standard number of retires until the program "gives up". I think 3-5 retires is a good amount. If there server is down we should catch that error, report back to the user so that they can try again at a later stage. ‚ùå

üí°Teacher feedback: You're mixing up scenarios! Let me clarify:

When to Use Exponential Backoff:

Scenario 1: Temporary network hiccup (USE exponential backoff)
Attempt 1: Wait 1 second ‚Üí Retry
Attempt 2: Wait 2 seconds ‚Üí Retry  
Attempt 3: Wait 4 seconds ‚Üí Retry
Attempt 4: Wait 8 seconds ‚Üí Give up

Why? The server might be momentarily overloaded. Backing off avoids hammering it.

Scenario 2: Server is completely down (DON'T use exponential backoff)

If connection refused or DNS fails ‚Üí Fail immediately, don't retry

Disk full:

"Ask user to make sure the URL is correct"

Reality check: This is often a batch process (no user to ask!). How would a production system handle this?
üí° Better approach:

Pre-check available disk space before starting
Reserve space for the download
Fail fast with clear error message

Checksum mismatch:
Great thinking, but asking the user mid-download is impractical.
Production approach:
# Pseudocode
if checksum_failed:
    if retry_count < max_retries:
        logger.warning(f"Checksum failed, attempt {retry_count}/{max_retries}")
        delete_file()
        retry_download()
    else:
        logger.error(f"Checksum failed after {max_retries} attempts. Corrupted source?")
        raise ChecksumValidationError()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


------------------------ Questions for You Before Stage 2: ------------------------

Testing Strategy: Should we write tests for the config parser before building the downloader logic? (TDD approach)
-> Yes i think it is a good idea to use the TDD approach as it helps catch bugs early and it makes sure I write clean reliable and testable code from the beginning of the project.

Resume Mechanism: Where should we store partial download state?
- A separate .progress file for each download?
- A SQLite database?
- JSON metadata files?
-> I am not sure what would be the best option for storing the partial download state. I have never worked with partial downloads before.


Thread Safety: If multiple threads write progress updates, what could go wrong? (Think about this now‚Äîit'll matter soon!)
-> We will need separate progress variables for each thread to update to make sure there is no overlap between the threads. Therefore we need ot make sure that there is proper synchronization of the threads. We need to be award of race conditions, data corruption, memory inconsistency, inaccurate reporting, and performance degradation. 

------------------------ Questions for Stage 2: Great Responses! ------------------------

TDD Approach: ‚úÖ Correct! We'll write tests first.
Resume Mechanism: You're honest about not knowing - perfect!

Teacher answer:

SQLite = overkill for this project
JSON metadata = my recommendation

// downloads/.progress/cifar10.json
{
  "url": "https://...",
  "total_size": 170498071,
  "downloaded_bytes": 50000000,
  "chunks_completed": [0, 1, 2],
  "chunks_pending": [3, 4, 5, 6, 7],
  "last_updated": "2025-10-01T10:30:00Z"
}

Why JSON:

Human-readable (debugging!)
Easy to delete/reset
No schema setup needed
Fast reads/writes

Thread Safety: ‚úÖ You identified the key issues!
One critical point you missed:
# ‚ùå NOT thread-safe:
progress = 0
def download_chunk():
    progress += chunk_size  # Race condition!

# ‚úÖ Thread-safe:
from threading import Lock
progress_lock = Lock()
progress = 0

def download_chunk():
    with progress_lock:
        progress += chunk_size


1. Should validation happen in load_config() or separate function? Why?
--> I think we should create a separate validate_config() function, this makes the code more modular and testable since we can tweak the validate_config() function to check various validation steps without adding to many steps and lines of code to the load_config() function. We can create a decorator function validate_config() and wrap the laod_config() fucntion with the decorator to perform the checks for us. 

2. How would you implement exponential backoff for retries?
   (Write pseudocode)

  def exp_backoff(action_function, max_retries, initial_delay_seconds):
    current_attempt = 0
    current_delay = initial_delay_seconds

    while current_attempt < max_retries:
        try:
            result = action_function()  // Execute the desired action
            RETURN result  // Action succeeded
        except Exception AS e:
            PRINT "Attempt " + (current_attempt + 1) + " failed: " + e.message

            IF current_attempt == max_retries - 1:
                RAISE Exception("Maximum retries reached, operation failed.")

            jitter = RANDOM.uniform(0, 1)  // Add random jitter
            sleep_time = current_delay + jitter
            PRINT "Retrying in " + sleep_time + " seconds..."
            SLEEP sleep_time

            current_delay = min(current_delay * 2, max_delay)  // Double the delay for the next attempt
            current_attempt = current_attempt + 1

    RAISE Exception("Unexpected error: Should have either returned or raised earlier.")


3. Design the JSON structure for resume files - what fields do you need?
--> {
    "dataset_name": "cifar10",
    "url": "https://...",
    "destination": "downloads/cifar10/cifar-10-python.tar.gz",
    "total_size_bytes": 170498071,
    "downloaded_bytes": 85249036,
    "download_strategy": "chunked",
    "chunks": {
        "chunk_size_bytes": 10485760,
        "total_chunks": 17,
        "completed_chunks": [0, 1, 2, 3, 4, 5, 6, 7],
        "pending_chunks": [8, 9, 10, 11, 12, 13, 14, 15, 16]
    },
    "last_updated": "2025-10-01T14:30:00Z",
    "checksum": "c58f30108f718f92721af3b95e74349a",
    "checksum_type": "md5"
}

Why these fields?

completed_chunks: Resume knows exactly where to continue
last_updated: Detect stale resume files (older than 24 hours?)
destination: Handle absolute vs. relative paths

4. Where should resume files be stored?
   - Same folder as downloads?
   - Separate .progress/ folder?
   - Why does it matter?
   --> I think resume files should be stored in a separate .progress/ folder so that they are easy to find and not confused with the already downloaded files. 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

------------------------ Finding Dataset Information ------------------------

How to Find Dataset URLs, Sizes, Checksums, and Formats:General Process:

Go to the dataset's official source (Kaggle, UCI ML Repository, Hugging Face, university sites)
Look for download links - usually on a "Download" or "Data" page
Check documentation - often has checksums and file info
Use command-line tools to verify after download (I'll show you)
Tools for Verification:

# Get file size (after downloading manually once)
ls -lh filename.tar.gz

# Calculate MD5 checksum
md5sum filename.tar.gz

# Calculate SHA256 checksum  
sha256sum filename.tar.gz

# Check archive format
file filename.tar.gz  # Shows "gzip compressed data"

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Q1: In DatasetConfig, why do we need both `url` and `urls` fields?
--> We need url for datasets that only have one file to download while we need the pural "urls" for datasets that contain multiple files to download

Q2: What happens if someone puts both 'url' and 'urls' in YAML? 
    Should validate_dataset_config() allow this?
--> Rule: Either use url (single file) OR urls (multiple files), never both!

# Validation logic:
has_url = 'url' in dataset_dict
has_urls = 'urls' in dataset_dict

if has_url and has_urls:
    raise ValueError(f"Dataset '{name}' has both 'url' and 'urls'. Use only one.")
if not has_url and not has_urls:
    raise ValueError(f"Dataset '{name}' missing both 'url' and 'urls'. Need one.")

Q3: Should file_size validation check against actual file size during download?
    Or only validate it's a positive number in config?
--> It should validate that it is a positive number as well as against the actual file size during download.

Q4: If checksum is "skip", should we still validate the format?
-->  No! If someone writes checksum: "skip", we shouldn't validate it's a valid MD5/SHA256. It's a special keyword meaning "don't validate this file."

if checksum.lower() != "skip":
    # Only then validate it's proper hex format
    validate_checksum_format(checksum, checksum_type)

What You've Accomplished So Far

1. ‚úÖ Project structure set up
2. ‚úÖ YAML configuration designed with real datasets
3. ‚úÖ Config loader implemented with comprehensive validation
4. ‚úÖ 12 passing unit tests using TDD approach
5. ‚úÖ Understanding of fixtures, regex patterns, and test isolation

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

---------------------- Stage 3.1: Logging System ----------------------


Why Logging Matters in ML/AI

In production ML pipelines:
- Downloads can run for hours
- Multiple datasets download concurrently
- Debugging failures requires detailed logs
- You need searchable, structured logs for auditing

---------------------- Question 1: Log Levels ---------------------- 

For each scenario, what log level should you use?

a) Download started for CIFAR-10
   ‚Üí Level: INFO (DEBUG/INFO/WARNING/ERROR)

b) Network timeout occurred, retrying (attempt 2/3)
   ‚Üí Level: WARNING

c) Checksum validation failed after 3 attempts
   ‚Üí Level: ERROR

d) Thread spawned to download chunk 5/10
   ‚Üí Level: INFO

e) Download completed successfully in 45.2 seconds
   ‚Üí Level: INFO ‚ùå
   This should be DEBUG. Here's why:
    INFO = significant events users care about ("Download started", "Download complete")
    DEBUG = detailed diagnostic info for developers ("Thread spawned", "Chunk 5 received")

---------------------- Question 2: Log Format ----------------------

Design a log message format. What information should EVERY log line include?

Consider:
- Timestamp?
- Thread ID (for debugging concurrent downloads)?
- Dataset name?
- Log level?
- Message?

Write an example log line for "Started downloading CIFAR-10"

--> log_format = '%(asctime)s - [%(levelname)s] - [Thread-%(thread)d] - %(message)s'


---------------------- Question 3: Log Files ----------------------

Should you have:
a) One log file for everything: downloader.log
b) Separate log files per dataset: cifar10.log, mnist.log
c) Both console output AND file output?

Why? What are the tradeoffs?
I think we should have separate log files for each indivdual dataset. This would make searching for the log files of each dataset easier and it will help with debugging each dataset and the log files. I am not sure if we should have both console and file output and I am not sure what the tradeoffs are. ‚ùå
****
Problem: If you download 50 datasets, you get 50 log files. Now try to debug why downloads are slow overall - you'd need to check 50 files!
Better approach: Both console AND file output

File: Everything goes to logs/downloader.log (searchable, persistent)
Console: INFO+ messages (so you see progress while it runs)

Tradeoffs:

One file = Easy to search across all downloads, but large file
Per-dataset files = Isolated debugging, but can't see big picture
Both console + file = Best of both worlds (industry standard)

---------------------- Question 4: Thread Safety ----------------------

Multiple threads will write logs simultaneously.

Is Python's logging module thread-safe by default?
Research this and write your answer.

If yes, do you still need locks around logging calls?

--> After researching i found that Python's logging module is thread-safe and does not require thread locks around logging calls if we make use of of the standard Logger and Handler instance on the different threads. Therefore, the logging module should handle the necessary synchronisation to prevent race conditions. 

----- Key points for logger.py -----

1. RotatingFileHandler prevents logs from growing forever
2. Console = INFO+, File = DEBUG+ (different levels)
3. Thread ID in format for debugging concurrent downloads
4. Check for existing handlers to prevent duplicates



 Stage 2: Logging System (Complete)

‚úÖ 1. Thread-safe logging with proper formatting
‚úÖ 2. Console + file output with different levels
‚úÖ 3. Rotating file handler (prevents infinite growth)
‚úÖ 4. 5 passing tests for logging

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

-------------------------- Stage 3 - Downloader --------------------------

--- Question 1: HTTP Request Design ---
When downloading a file, what information do you need?

1. From the HTTP response headers, which fields matter?
   - Content-Length: The content-length returns the size of the response body whihc is important for our validation process to make sure that the file we are downloading matches the length (size) of the file we have in out config.yaml module. 
   - Content-Type: content-type specifies the data type of the data is being downloaded which is importatnt to know.Helps with logging
   - Accept-Ranges: The accept-ranges response header is important for out resume capability as it allows clients to request partial content instead of the entire resource. This is important so that we can request the content from the point of resuming the download. 

2. What HTTP response codes should you handle differently?
   - 200: the request was successful so we should log this and continue with the program, no special handling needed.
   - 206: response to a range request when the client has requested a part or parts of a resource. This does require special handling as it relates to the resume capability of our program. 
   - 404: The server cannot find the requested resource. This means that the browser cannot find the URL we want to request from. We need to log this and then make sure that the URL we have in our config.yaml file is correct.
   - 403: Fail immediately
   - 500: The server has encountered a situation it does not know how to handle. We should handle this by logging the server error then performing our exponential backoff retry strategy. 
   - 503: The server is not ready to handle the request. We should log this error and then perform out exponential backoff retry strategy. 

3. Should you download the entire file into memory then write?
   Or stream chunks and write incrementally?
   Why? (Consider a 10GB file)
   --> I think it is better to stream the data in chunks and then write incrementally to avod overloading the device memory and slowwing down the process or even crashing the device altogether. 

--- Question 2: Retry Logic Design ---

When should you retry vs. give up?

1. Network timeout after 30 seconds
   - Retry? yes, the number of retries should depend on the exponential backoff algorithm
   - Wait how long between retries? again this would depend on the exponential backoff algorithm, but we start at around 2 seconds and tthen can backoff to around 8 seconds. 

2. HTTP 404 (Not Found)
   - Retry?  No
   - Why?
   This indicates that the URL we used for the request cannot be found by the browser so if we keep on retrying we wont get a different result so we should log the issus and then nottify the client to check the URL is correct. 

3. HTTP 503 (Service Unavailable)
   - Retry? Yes
   - Why?
   The service my just be Temporarily Unavailable so we can use our exponential backoff stratgey to retry a certain number of times before stopping.

4. Exponential backoff calculation:
   - Attempt 1: Wait 2 seconds
   - Attempt 2: Wait 4 seconds
   - Attempt 3: Wait 8 seconds
   - Formula: delay = min(base_delay * (2 ** attempt), max_delay)

5. What's the maximum wait time (backoff cap)?
   Why do you need a cap?
   We need a backoff cap so that we prevent waiting a long amount of time between retries  with the exponential backoff algorithm. 

--- Question 3: Progress Tracking ---

Progress bars need to update as data arrives.

1. How often should progress update?
   - Every byte? This could cause a big overhead of compute power for just tracking the progress of the download.
   - Every chunk? Update every chunk received from the network, typically 8KB-64KB. This is already chunked by requests.iter_content(chunk_size=8192).

2. What information does a progress bar need?
   - Total size (bytes)
   - Downloaded so far (bytes)

3. If the server doesn't provide Content-Length, can you show progress?
   What would you show instead?
   --> I don't think we would be able to show the progress if the server doesn't provide the content length. Then we could possible should the number of bytes downloaded or the number of chunks downloaded. 

--- Question 4: Error Handling Strategy --- 

What errors can occur during download?

1. Connection errors (network unreachable):
   Action: log the issue then Use the exponential backoff strategy to retry connecting to the network. Check to make sure the client is connected to the internet.

2. Timeout errors (server doesn't respond):
   Action: log the error then Use the exponential backoff strategy to retry a ceratin number of times while notifying the client before stopping at the backoff cap and notfying the client about the error.

3. HTTP errors (4xx, 5xx status codes):
   Action: 4xx (Client errors like 404, 403): Don't retry - client's fault
           5xx (Server errors like 500, 503): Retry - server's fault

4. Disk full while writing:
   Action: 1) Check available disk space BEFORE downloading. 2) If insufficient, fail immediately with clear error. 3) User decides what to delete, not your program

5. Incomplete download (received < expected):
   Action: 1) Log the mismatch 2) Fail the download 3) If resume is enabled, next run will continue from where it stopped

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

-------------------------- Stage 4 - Resume Capability & Progress Persistence --------------------------

------------- Question 1: HTTP Range Headers  -------------

 HTTP Range headers let you request specific byte ranges of a file.

1. If a file is 10,000 bytes and you've downloaded 6,000 bytes,
   what Range header do you send?
   Range: bytes= 6000- (open ended dash means from xxxx to end)
   
2. What HTTP status code indicates the server supports ranges?
   Status: 206
   
3. What HTTP status code do you get for a range request?
   Status: 206
   
4. If the server doesn't support ranges, what happens?
   a) Request fails
   b) Server sends entire file (status 200)
   c) Server sends error
   Answer: b - Server sends entire file (status 200)
   
5. Where do you check if server supports ranges?
   Response header: accept-ranges or Accept-Ranges (capitalization matters in some tools, though HTTP headers are case-insensitive)

------------- Question 2: Progress File Design -------------

You need to save download state between runs.

1. What information must be saved to resume?
   - URL: Yes 
   - Downloaded bytes: Yes 
   - Total size: Yes 
   - Destination path: Yes 
   - Checksum: No
   - What else? --> Last modified timestamp: To detect if download was stale
C                   Checksum type: To know which algorithm to use for validation

2. Where should progress files be stored?
   a) Same folder as download
   b) Separate .progress/ folder
   c) /tmp directory
   Answer: in a separate .progress/ folder Why? --> this helps keep the downloads folder clean and allows for automatic cleanup of Temporary progress files
   
3. Progress file naming:
   If downloading to "downloads/cifar10/data.tar.gz"
   Progress file should be: .progress/cifar10/data.tar.gz.progress
   Why? --> If you download multiple files to cifar10/ folder, they'd overwrite each other's progress!
   
4. When should progress files be deleted?
   - After successful download: No
   - After validation passes: Yes
   - After extraction: Yes
   - Never (user deletes manually): No

5. What if the destination file exists but progress file doesn't?
   Should you:
   a) Resume from file size
   b) Start fresh (overwrite)
   c) Ask user
   Answer: b - Start fresh (overwrite)
Why? Without a progress file, you don't know:
- What URL was being downloaded
- If the file is corrupted
- If it's even the right file

Starting fresh is safer.

------------- Question 3: Resume Logic -------------

Design the resume workflow:

1. Start of download:
   - Check if progress file exists
   - If yes: Load downloaded_bytes
   - Check if partial file exists
   - If file size != downloaded_bytes ‚Üí 

Action: Delete partial file and start fresh
Reason: File size mismatch indicates:
- File was manually edited
- Previous download crashed mid-write
- Filesystem corruption
Resume would append to wrong position = corrupted file
   
2. Making the request:
   - If resuming: Add Range header
   - If not resuming: Regular GET request
   
3. During download:
   - How often should progress file update?
     Every chunk? Every 10 chunks? Every MB? --> Every 1-5 MB OR every 10 seconds (whichever comes first)
   - Why not update every chunk? 
Why? Balance between:
- Frequent updates = more accurate resume point, but more disk I/O
- Infrequent updates = less overhead, but lose more progress on crash
Every 8KB chunk = 100,000s of file writes per GB!
Every 1MB = reasonable compromise
   
4. After download completes:
   - Delete progress file: No
   - Keep it for validation: Yes
   
5. Error during resume:
   - What if server now returns 404? --> Delete progress file, delete partial file, fail immediately (URL is dead)
   - What if file size changed? -- Delete progress file, delete partial file, start fresh (Server's file was replaced/updated)
   - What if Range request fails? --> (server returns 200 instead of 206): Log warning, delete partial file, start fresh download (Server doesn't support resume)

------------- Question 4: File Append Mode -------------

When resuming, you need to append to existing file.

1. What open() mode for appending binary data?
   Mode: "ab" mode
   
2. If you open in 'wb' mode when resuming, what happens?
   Answer: you will overwrite the file and lose all previously saved data.
   
3. Before appending, should you validate the partial file?
   - Check file size matches progress: Yes
   - Check partial checksum: No (Hard!)
   
4. What if partial file is corrupted?
   Should you:
   a) Resume anyway
   b) Delete and start fresh
   c) Try to detect corruption
   Answer: b - Delete and start fresh
Why? Detecting corruption requires:
- Checksumming entire partial file (expensive for GB files)
- Knowing expected checksum for partial data (almost never available)
Practical approach: If file size mismatches progress, assume corruption and restart.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

-------------------------- Stage 5 - Checksum Validation & File Integrity --------------------------

------------- Question 1: Checksum Algorithms  -------------

1. What's the difference between MD5 and SHA256?
   - MD5 output size: _____ (bits/bytes)
   - SHA256 output size: _____ (bits/bytes)
   - Which is more secure? _____
   - Which is faster? _____
   - When to use each? _____

2. What is a hash collision?
   Definition: _____
   
   Why does it matter for validation?
   _____

3. If you compute MD5 of a 10GB file twice, will you get:
   a) Different hashes each time
   b) Same hash both times
   c) Depends on file content
   Answer: _____

4. Can you tell file size from a checksum?
   Yes / No
   Why? _____

------------- Question 2: When to Validate  -------------

1. Should you validate DURING download or AFTER complete?
   Option A: Calculate checksum as chunks arrive
   Option B: Read entire file after download completes
   
   Trade-offs:
   - Memory usage: _____
   - Detection speed: _____
   - Implementation complexity: _____
   
   Your choice: _____ Why? _____

2. What if validation fails?
   a) Retry download immediately
   b) Delete file and fail
   c) Keep file but log error
   Answer: _____ Why? _____

3. Should you validate if checksum is 'skip'?
   Yes / No
   
4. What if user provides wrong checksum?
   - Downloaded file is actually correct
   - But validation fails (wrong expected hash)
   How do you handle this?

------------- Question 3: Partial File Validation -------------

Resume scenario: 
- Downloaded 5GB of 10GB file
- Program crashes
- Resume from 5GB

Question: Can you validate the partial 5GB file?
   Yes / No
   
If yes, how? _____
If no, why not? _____

Industry approach:
- Validate during download: Yes / No
- Validate partial files: Yes / No
- Validate complete file only: Yes / No

------------- Question 4: Performance Considerations -------------

Checksumming a 10GB file:

1. If you read entire file into memory:
   Memory usage: _____
   Problem: _____

2. If you read in chunks:
   Memory usage: _____
   How it works: _____

3. Typical checksum speed: ~500 MB/s
   Time to checksum 10GB: _____
   
4. Should checksumming block progress bar?
   Yes / No
   Why? _____

5. Can you checksum and download simultaneously?
   Yes / No
   How? _____